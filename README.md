# Sophia: Explaination and Expriments

We will be using the official implementation of the Sophia-G optimizer from the paper available at https://arxiv.org/abs/2305.14342 and the GPT-2 training scripts from the repository https://github.com/Liuhong99/Sophia.
A big thank you to Liu Hong, Li Zhiyuan, Hall David, Liang Percy, and Ma Tengyu for their hard work on this project üëè!

@article{liu2023sophia,
 title={Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training},
 author={Liu, Hong and Li, Zhiyuan and Hall, David and Liang, Percy and Ma, Tengyu},
 journal={arXiv preprint arXiv:2305.14342},
 year={2023}
}

# Simpleply explaination



# Experiment
GPT-2 was trained on OpenWebText with Sophia and Adam with one A6000 - 48GB GPU, 32GB and 7 cores CPU
